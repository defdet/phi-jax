{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_phi import Attention\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "from transformers import PhiConfig\n",
    "from transformers.models.phi.modeling_phi import PhiAttention, PhiDecoderLayer\n",
    "from modeling_phi import forward_attention, forward_decoder_block, forward_decoder, forward_embedding, pt2jax, jax2pt, convert_attention, convert_decoder_block, forward_layer_norm, forward_mlp\n",
    "from modeling_phi import phi_config, make_rotary_values, LayerNorm\n",
    "import torch.nn as tnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 7\n",
    "d_model = 32\n",
    "head_dim = 8\n",
    "n_heads_kv = 4\n",
    "n_rep_kv = 1\n",
    "vocab_size = 6\n",
    "d_ff = 37\n",
    "torch.manual_seed(1)\n",
    "\n",
    "config_pt = PhiConfig(hidden_size=d_model, num_attention_heads=n_heads_kv, vocab_size=vocab_size,layer_norm_eps=1e-05, intermediate_size=d_ff, hidden_act='gelu_new')\n",
    "config_jax = phi_config._replace(d_model=d_model, d_ff=d_ff, n_rep_kv=n_rep_kv, vocab_size=vocab_size, n_heads_kv=n_heads_kv, head_dim=head_dim, dropout_rate=None, partial_rotary_factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_jax [ 0.3809222   0.08554427 -0.32185036  0.09108333  0.21352716  0.21511878\n",
      "  0.02889613  0.28895354 -0.24973309  0.13385388 -0.38566226  0.12869063\n",
      "  0.00871488 -0.23161611 -0.30547583 -0.2266545  -0.2180866  -0.1230709\n",
      "  0.01938874 -0.32273737  0.13738525 -0.30827588 -0.01547235 -0.15963183\n",
      " -0.05082852 -0.09416683 -0.28768718 -0.2831776   0.08766142 -0.04026837]\n",
      "y_hat_jax [ 0.38092217  0.08554427 -0.32185036  0.09108337  0.21352716  0.21511877\n",
      "  0.02889613  0.2889536  -0.24973312  0.13385391 -0.38566223  0.12869063\n",
      "  0.00871487 -0.23161614 -0.30547583 -0.22665454 -0.21808663 -0.12307093\n",
      "  0.01938875 -0.3227374   0.13738526 -0.30827582 -0.01547235 -0.1596319\n",
      " -0.05082849 -0.09416679 -0.28768718 -0.28317758  0.08766145 -0.04026842]\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "attention_pt = PhiAttention(config=config_pt)\n",
    "params_jax = convert_attention(attention_pt, model_config=config_jax)\n",
    "\n",
    "\n",
    "\n",
    "# initialise input sequence\n",
    "seq_pt = torch.rand(batch_size, seq_len, d_model)\n",
    "seq_jax = pt2jax(seq_pt)\n",
    "\n",
    "mask_pt_1d = torch.ones(batch_size, seq_len, dtype=torch.bool)  # torch.rand(batch_size, seq_len) > 0.1\n",
    "mask_pt = torch.tril(torch.einsum('bi,bj->bij', mask_pt_1d, mask_pt_1d))[:, None]\n",
    "mask_jax_1d = pt2jax(mask_pt_1d)\n",
    "mask_jax = jnp.tril(jnp.einsum('bi,bj->bij', mask_jax_1d, mask_jax_1d))[:, None, None]\n",
    "leftpad_len = mask_jax_1d.argmax(axis=-1).astype(jnp.uint16)\n",
    "rotary_values = make_rotary_values(leftpad_len, batch_size, seq_len, model_config=config_jax)\n",
    "\n",
    "# In the Hugging Face implementation, the attention mask is added to the attention\n",
    "# matrix, not multiplied.\n",
    "# See https://github.com/huggingface/transformers/issues/1935\n",
    "mask_pt = torch.where(mask_pt, 0, -10000.)\n",
    "\n",
    "y_pt = attention_pt(hidden_states=seq_pt, attention_mask=mask_pt)[0]\n",
    "y_jax = pt2jax(y_pt)\n",
    "y_hat_jax, _ = forward_attention(params_jax, seq_jax, seq_jax, mask_jax,rotary_values=rotary_values, model_config=config_jax)\n",
    "\n",
    "y_jax = jnp.where(mask_jax_1d[..., None], y_jax, 0.)\n",
    "y_hat_jax = jnp.where(mask_jax_1d[..., None], y_hat_jax, 0.)\n",
    "\n",
    "print('y_jax', y_jax.reshape(-1)[:30])\n",
    "print('y_hat_jax', y_hat_jax.reshape(-1)[:30])\n",
    "assert jnp.allclose(y_jax, y_hat_jax)\n",
    "print('Test passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "embedding_pt = tnn.Embedding(config_jax.vocab_size, config_jax.d_model, -1)\n",
    "embedding_pt.weight = tnn.Parameter(torch.randn_like(embedding_pt.weight))\n",
    "\n",
    "params_pt = embedding_pt.weight\n",
    "params_jax = pt2jax(params_pt)\n",
    "\n",
    "x_pt = torch.tensor([[3, 3, 3, 0, 3, 2, 3, 1, 5]], dtype=torch.int)\n",
    "x_jax = pt2jax(x_pt).astype(jnp.uint16)\n",
    "\n",
    "y_pt = embedding_pt(x_pt)\n",
    "y_jax = pt2jax(y_pt)\n",
    "y_hat_jax = forward_embedding(params_jax, x_jax)\n",
    "assert jnp.allclose(y_jax, y_hat_jax)\n",
    "print('Test passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_jax [-1.4426247   0.31092632 -0.30887297  0.22166999 -2.1207542  -0.11964922\n",
      " -2.7633946   0.01631283  1.9290285   1.0304995  -0.5117087   3.478505\n",
      " -0.05939087  1.1803433   0.10975467  0.9459591  -1.6364973   0.03644655\n",
      "  1.298217    0.9042006   0.66670144 -0.50051916  1.0391926  -0.5723707\n",
      "  0.53466356 -0.06153665  1.0334806   2.3507762   0.07948992 -0.09832619]\n",
      "y_hat_jax [-1.4426247   0.31092635 -0.30887297  0.22166999 -2.1207542  -0.11964922\n",
      " -2.7633946   0.01631283  1.9290285   1.0304995  -0.5117087   3.478505\n",
      " -0.05939087  1.1803433   0.10975467  0.94595915 -1.6364973   0.03644655\n",
      "  1.2982172   0.9042006   0.66670144 -0.5005192   1.0391926  -0.5723707\n",
      "  0.53466356 -0.06153665  1.0334806   2.3507762   0.07948992 -0.09832621]\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 1\n",
    "\n",
    "norm_pt = tnn.LayerNorm(config_jax.d_model, eps=config_jax.layer_norm_epsilon, elementwise_affine=True)\n",
    "norm_pt.weight = tnn.Parameter(torch.randn_like(norm_pt.weight))\n",
    "\n",
    "params_pt = norm_pt.weight\n",
    "params_jax = LayerNorm(weight=pt2jax(norm_pt.weight), bias=pt2jax(norm_pt.bias))\n",
    "\n",
    "x_pt = torch.rand(batch_size, seq_len, config_jax.d_model)\n",
    "x_jax = pt2jax(x_pt)\n",
    "\n",
    "y_pt = norm_pt(x_pt)\n",
    "y_jax = pt2jax(y_pt)\n",
    "y_hat_jax = forward_layer_norm(params_jax, x_jax, model_config=config_jax)\n",
    "print('y_jax', y_jax.reshape(-1)[:30])\n",
    "print('y_hat_jax', y_hat_jax.reshape(-1)[:30])\n",
    "assert jnp.allclose(y_jax, y_hat_jax)\n",
    "print('Test passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_jax [ 0.20558816  0.24419892  0.1038277  -0.00047055  0.14069492 -0.2527915\n",
      "  0.13871008  0.02292301 -0.0296139  -0.11507002  0.06133269  0.02876599\n",
      "  0.11477372  0.27442294  0.05185471  0.08392327 -0.03465857  0.1597825\n",
      " -0.05245028 -0.24720992  0.02974929 -0.10189229  0.14556657 -0.09291685\n",
      "  0.02649859 -0.3545338  -0.09292045  0.02840933  0.28200686  0.31341866\n",
      "  0.0162026   0.13692412  0.30714723  0.25758561  0.04955429 -0.08822581\n",
      "  0.1308631  -0.15604994  0.19415739  0.10688122  0.09553957 -0.02963576\n",
      " -0.04539628  0.07708994  0.02144855  0.24398202 -0.00731712  0.06229361\n",
      " -0.05898028  0.1293217  -0.00369786 -0.13411754 -0.00721333 -0.0674806\n",
      "  0.10203804  0.00868339 -0.02300387 -0.41825277 -0.07009862  0.06579411]\n",
      "y_hat_jax [ 0.20558454  0.24420096  0.1038224  -0.00046931  0.14071405 -0.25279364\n",
      "  0.1387099   0.02293312 -0.02960952 -0.11508399  0.06134358  0.0287626\n",
      "  0.11477291  0.2744342   0.05186819  0.08393145 -0.03464745  0.15980378\n",
      " -0.05245134 -0.24722143  0.02976028 -0.10190032  0.14555252 -0.09290731\n",
      "  0.02649837 -0.3545459  -0.0929126   0.02841108  0.28201938  0.31342286\n",
      "  0.0162201   0.13692307  0.30714855  0.25758627  0.0495382  -0.08822159\n",
      "  0.13090533 -0.15604813  0.19416453  0.10688664  0.09555389 -0.02965157\n",
      " -0.04539084  0.07708912  0.0214588   0.24400204 -0.0073048   0.06229011\n",
      " -0.05895377  0.1293388  -0.00369719 -0.13413857 -0.00719451 -0.06747894\n",
      "  0.10202894  0.00870892 -0.02300355 -0.41828328 -0.07009254  0.06578615]\n"
     ]
    }
   ],
   "source": [
    "import einops as op\n",
    "decoder_block_pt = PhiDecoderLayer(config=config_pt, layer_idx=1)\n",
    "params_jax = convert_decoder_block(decoder_block_pt, model_config=config_jax)\n",
    "mlp_pt = decoder_block_pt.mlp\n",
    "seq_pt = torch.rand(batch_size, seq_len, d_model)\n",
    "seq_jax = pt2jax(seq_pt)\n",
    "\n",
    "y_pt = mlp_pt(seq_pt)\n",
    "y_jax = pt2jax(y_pt)\n",
    "\n",
    "y_hat_jax = forward_mlp(params_jax, seq_jax)\n",
    "\n",
    "print('y_jax', y_jax.reshape(-1)[:60])\n",
    "print('y_hat_jax', y_hat_jax.reshape(-1)[:60])\n",
    "\n",
    "assert jnp.allclose(y_jax, y_hat_jax, rtol=1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_jax [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.37413242 -0.324925   -0.04732257 -0.22331363\n",
      "  1.1843688   0.16076326  0.9275273  -0.80919784  0.4814498   0.94506735\n",
      " -0.81111825  0.2613848   0.5426444   1.4842494   0.9145651   0.49251202\n",
      "  0.5494744   0.8769037   0.65794027  0.49175996  0.5358275   0.44394332\n",
      "  1.3008204   1.3742481  -0.33338895 -0.27109402  0.61694956  1.1081872 ]\n",
      "y_hat_jax [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.37409496 -0.3249396  -0.04736719 -0.22331324\n",
      "  1.1843626   0.16073757  0.9275802  -0.80928105  0.48140275  0.9450928\n",
      " -0.8111192   0.26136982  0.54267     1.4843102   0.91459954  0.49253932\n",
      "  0.5495163   0.8769436   0.6579074   0.49174678  0.53582096  0.44389957\n",
      "  1.3009009   1.3742446  -0.3333846  -0.27110946  0.6169773   1.1081891 ]\n"
     ]
    }
   ],
   "source": [
    "import einops as op\n",
    "decoder_block_pt = PhiDecoderLayer(config=config_pt, layer_idx=0)\n",
    "params_jax = convert_decoder_block(decoder_block_pt, model_config=config_jax)\n",
    "\n",
    "seq_pt = torch.rand(batch_size, seq_len, d_model)\n",
    "seq_jax = pt2jax(seq_pt)\n",
    "\n",
    "mask_pt_1d = torch.rand(batch_size, seq_len) > 0.7\n",
    "mask_pt = op.rearrange(torch.tril(op.einsum(mask_pt_1d, mask_pt_1d, 'B L1, B L2 -> B L1 L2')), 'B L1 L2 -> B 1 L1 L2')\n",
    "mask_jax_1d = pt2jax(mask_pt_1d)\n",
    "mask_jax = pt2jax(mask_pt)\n",
    "\n",
    "leftpad_len = mask_jax_1d.argmax(axis=-1).astype(jnp.uint16)\n",
    "rotary_values = make_rotary_values(leftpad_len, batch_size, seq_len, model_config=config_jax)\n",
    "mask_pt = torch.where(mask_pt, 0, -10000000.)\n",
    "\n",
    "y_pt = decoder_block_pt(hidden_states=seq_pt, attention_mask=mask_pt)[0]\n",
    "y_jax = pt2jax(y_pt)\n",
    "y_hat_jax = forward_decoder_block(params_jax, seq_jax, mask_jax, rotary_values=rotary_values, model_config=config_jax)[0]\n",
    "\n",
    "y_jax = jnp.where(mask_jax_1d[..., None], y_jax, 0.)\n",
    "y_hat_jax = jnp.where(mask_jax_1d[..., None], y_hat_jax, 0.)\n",
    "\n",
    "print('y_jax', y_jax.reshape(-1)[:60])\n",
    "print('y_hat_jax', y_hat_jax.reshape(-1)[:60])\n",
    "\n",
    "assert jnp.allclose(y_jax, y_hat_jax, rtol=1e-02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
